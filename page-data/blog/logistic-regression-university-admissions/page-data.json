{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/logistic-regression-university-admissions","result":{"pageContext":{"content":"<p>Machine Learning on Coursera Week 2 assignment involves building a logistic regression model to predict whether a student will be offered admissions to a university based on two test scores.</p>\n<h2>Part 1: Getting the Data</h2>\n<p>As usual, we first load the dataset into a Python array.</p>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib\nimport math\nimport scipy.optimize as op\n\npath = '/Users/archit-p/sources/coursera/machine-learning/machine-learning-ex2/ex2/ex2data1.txt'\n</code></pre>\n<pre><code class=\"language-python\">f = open(path, 'r')\n\ndataset = []\n\nfor line in f:\n    fields = line.strip().split(',')\n    fields[0] = float(fields[0])\n    fields[1] = float(fields[1])\n    fields[2] = int(fields[2])\n    dataset.append(fields)\n</code></pre>\n<p>Each row of data consists of two test scores and a numeric value indicating which is 1 if the student was admitted and 0 otherwise.</p>\n<pre><code class=\"language-python\">dataset[0:5]\n</code></pre>\n<pre><code>[[34.62365962451697, 78.0246928153624, 0],\n [30.28671076822607, 43.89499752400101, 0],\n [35.84740876993872, 72.90219802708364, 0],\n [60.18259938620976, 86.30855209546826, 1],\n [79.0327360507101, 75.3443764369103, 1]]\n</code></pre>\n<p>We transform the data into a <code>numpy</code> matrix with features and a <code>numpy</code> vector with results.</p>\n<pre><code class=\"language-python\"># number of features\nn = 2\n\n# number of datapoints\nm = len(dataset)\n\nx = np.ones(shape=(m, n + 1))\ny = np.zeros(m)\n\nfor i, d in enumerate(dataset):\n    for j in range(n):\n        x[i][1 + j] = dataset[i][j]\n        y[i] = dataset[i][-1]\n</code></pre>\n<pre><code class=\"language-python\">x[0:5]\n</code></pre>\n<pre><code>array([[ 1.        , 34.62365962, 78.02469282],\n       [ 1.        , 30.28671077, 43.89499752],\n       [ 1.        , 35.84740877, 72.90219803],\n       [ 1.        , 60.18259939, 86.3085521 ],\n       [ 1.        , 79.03273605, 75.34437644]])\n</code></pre>\n<pre><code class=\"language-python\">y[0:5]\n</code></pre>\n<pre><code>array([0., 0., 0., 1., 1.])\n</code></pre>\n<p>We can visualize the data now using <code>matplotlib</code>.</p>\n<pre><code class=\"language-python\">plt.scatter(x[:, 1], x[:, 2], c=y, cmap='viridis')\nplt.show()\n</code></pre>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 375px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4e6a9dfb34f8340072724b3f1fd22243/5ff7e/assignment-2-output_11_0.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAChUlEQVQ4y22T3W7TMBTHK76EEI/GDfcb7CW44zFgV/AOgwcYMJVWpe02wkC02tqua9M1seMk7eKP2OdwnGQTSFg6+vv4uD+ff+y2Wq3W08PPh8+zJBuNTqbftWKBLHggVRlIqUhdIG8mtCZIZ6TXtJ5SbJq6DpTSJ0VRzMMwfN1CxNZpcLqnC41bUVCqEcGrq+doKeekJS2tKZ1QXNB8hfVQ1R4AwCRJ3lXATqez21TLmrC2YLkFJykyitwCGAptwfy2oHoWaB8xKB9TfatcGSHj4m0FbLfbLz2NhYlTNwpElFd0gBLBq8sRyvNabVTX/LxuAtAJ4yjhPNmvgL1eb8dXRv1zdxHMCJhVlsHWtkB9JYBoQFsE85PirD4MDCBI48oV8viyBvb7/Z3GsouuGGQsR1WUWKoVWsNqCHUGLqXYEGzcONDU+QUZXxunAuQs3P/HMsGcUQa01Oiho8EM9XbanFWgKxnejgrmbYOuO7Q5AaM7y7ulKbG5Wig2EleTNXrr2zTHjAvcZpJKCUHKhkidllNvm+4GjaN1zlkNHBwPXlyNQ5z+uHRGl9W39h12Pvar37JlgnyVYLyIKaOuzAiNXNBnSKmFa9ovjH82jPEaODwd7EZzhtOzufHPBhxYr2TdpnFm82Rj2ZLbxXhl5VZauWH2/PibtVpYU0wtD5fSaOPf4ZvbS9nD/wwCY0jWb+d+aGlQK3u3x1n/3FKkb49plr6vgAcfDp7FUTwUifjCGOvQSW0hRDuO406e50ek3SzLjjjn3cVs2Z5PrtqJ4J00FVSLulIVn6I4+jUej1/VlodD+ku3HhPM60O/5oPGg0afzOdzr/f/qj2ii6xqLIsrpbj3B4hslxUt52QhAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/4e6a9dfb34f8340072724b3f1fd22243/5ff7e/assignment-2-output_11_0.png\"\n        srcset=\"/static/4e6a9dfb34f8340072724b3f1fd22243/772e8/assignment-2-output_11_0.png 200w,\n/static/4e6a9dfb34f8340072724b3f1fd22243/5ff7e/assignment-2-output_11_0.png 375w\"\n        sizes=\"(max-width: 375px) 100vw, 375px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Part 2: Logistic Regression and Gradient Descent</h2>\n<p>We will define a helper function to calculate the sigmoid function for a given parameter.</p>\n<pre><code class=\"language-python\">def sigmoid(z):\n    return 1 / (1 + math.exp(-z))\n</code></pre>\n<pre><code class=\"language-python\">sigmoid(0)\n</code></pre>\n<pre><code>0.5\n</code></pre>\n<p>Hypothesis and mean square error functions can be calculated given the feature vectors xi and the parameters theta. We define an additional function gradient which will be used for calculating the gradient given theta, x and y.</p>\n<pre><code class=\"language-python\">def hypothesis(theta, xi):\n    return sigmoid(np.matmul(np.transpose(theta), xi))\n</code></pre>\n<pre><code class=\"language-python\">def mse(theta, x, y):\n    sum = 0\n    for i in range(len(x)):\n        sum -= y[i] * math.log(hypothesis(theta, x[i, :])) + (1 - y[i]) * math.log(1 - hypothesis(theta, x[i, :]))\n    return sum / m\n</code></pre>\n<pre><code class=\"language-python\">def gradient(theta, x, y):\n    res = np.empty(n + 1)\n    for j in range(n + 1):\n        sum = 0\n        for i in range(len(x)):\n            sum += x[i][j] * (hypothesis(theta, x[i, :]) - y[i])\n        res[j] = sum / m\n    return res\n</code></pre>\n<pre><code class=\"language-python\">mse(np.zeros(3), x, y)\n</code></pre>\n<pre><code>0.6931471805599458\n</code></pre>\n<pre><code class=\"language-python\">gradient(np.zeros(3), x, y)\n</code></pre>\n<pre><code>array([ -0.1       , -12.00921659, -11.26284221])\n</code></pre>\n<pre><code class=\"language-python\">result = op.minimize(fun = mse,\n                        x0 = np.zeros(n + 1),\n                        args = (x, y),\n                        method = 'TNC',\n                        jac = gradient);\n\ntheta = result.x\n</code></pre>\n<pre><code class=\"language-python\">hypothesis(theta, np.array([1, 45, 85]))\n</code></pre>\n<pre><code>0.7762906253511527\n</code></pre>\n<p>Using the minimized theta values, lets plot the decision boundary.</p>\n<pre><code class=\"language-python\">c = -theta[0] / theta[2]\nm = -theta[1] / theta[2]\n\nxmin, xmax = 30, 100\nxd = np.array([xmin, xmax])\nyd = xd * m + c\n\nplt.scatter(x[:, 1], x[:, 2], c=y, cmap='viridis')\nplt.plot(xd, yd, 'k', lw=1, ls='--')\nplt.show()\n</code></pre>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 375px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b549b9378fefc2dea3ef3ca3f3e54a94/5ff7e/assignment-2-output_24_0.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAClklEQVQ4y3VTzW7TQBA2P5UQgkdD4gVogXuP3HgM6AmeoSeOqCQpTaS2BFKK2iRNmqRxYnttrx17d+39GWY3CXBhpPG3M+P99pvx2vM878mX5tFzSuj19bfRj7oiPV7GPS50j3OBaDAe9ThLEceIAeYper6pq54QVZcxNvV9/60HAF73e/dVkRUQBymGNXoJYBiidYFrgqgA9AJAXqFf4HoJzkzi9hijIUmSD46w1Wzt2poGLRlnipczBaZAl8roAj1XRiWIK2WqnjL1pTJ4ApIoI6c2L7QkQGL63hE2m82XlrB/fqODaWQm13MQogRZ0bUIOUIfowoBRmdrlAPYmAGd1BpPiEl44AjbJ+0XthIvUj26mJg8XoGUFQg2AM5XYMQpyk9cW0anGLfBqGBNrEIkzGotukg4WxN2Oh1HmCcrTaPcFLSEVVrC+GIAq+QSeKVQLc5M+U4QjgGfsEblG8zXup5AHJODTcst17JglbbvlTmDYBoC8XMQZQy8yCFLMxxDAMpOzzWKyuWtJTbotdbVX8LT89Pd+WABdwNf15V0O4JJBJ1PZ6AwXN5GQOYpJMs7YGyBB/RB16hWx0hH8f2itqyEbAjPumd7/s0Sht2xU7idNg0zoFFmRwHhjMD0aomKS8jIBH6efIaKRyDZwKSLaY1mr82asNFo7G048CSjrIO7FkbNhwuHSmmXY4VQFdcY45oJlVGmwlnCa4GEafJu+1Feu7HoP+L+a7zgzremtARRcbwV0ir8iH+ed//w8PAZ9n9OKW0gnmDhOE3TY7vGXDOKonaWZc0wDNtZnh1Tmn7FnKuRiLQRj2az2a/hcPjG29/fv2dVoj2K49jijo03uYcbfIwbLD74p7azrfX7fYtP7eM36rWQ6Eh0YoAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/b549b9378fefc2dea3ef3ca3f3e54a94/5ff7e/assignment-2-output_24_0.png\"\n        srcset=\"/static/b549b9378fefc2dea3ef3ca3f3e54a94/772e8/assignment-2-output_24_0.png 200w,\n/static/b549b9378fefc2dea3ef3ca3f3e54a94/5ff7e/assignment-2-output_24_0.png 375w\"\n        sizes=\"(max-width: 375px) 100vw, 375px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Having visualized the decision boundary, let us now see how accurate it is at predicting the outcomes.</p>\n<pre><code class=\"language-python\">def predict(theta, x):\n    predictions = np.zeros(len(x))\n    for i in range(len(x)):\n        predictions[i] = hypothesis(theta, x[i]) >= 0.5\n    return predictions\n</code></pre>\n<pre><code class=\"language-python\">def accuracy(theta, x, y):\n    predictions = predict(theta, x)\n    cnt = 0\n    for i in range(len(y)):\n        cnt += y[i] == predictions[i]\n    return cnt / len(y)\n</code></pre>\n<pre><code class=\"language-python\">accuracy(theta, x, y)\n</code></pre>\n<pre><code>0.89\n</code></pre>\n<p>We've achieved an 89% accuracy for the model. Looking at the decision boundary above, this might be the most accurate we can get using a straight line.</p>\n<p>That was it on this assignment. We've seen how to use gradient descent to train a logistic regression model. The main difference here from a linear regression was the <code>gradient</code> function.</p>","frontmatter":{"slug":"/blog/logistic-regression-university-admissions","title":"Logistic Regression to Predict University Admissions","date":"June 13th 2021"}}},"staticQueryHashes":[]}