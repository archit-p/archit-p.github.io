{"componentChunkName":"component---src-templates-blog-template-js","path":"/blog/linear-regression-housing-prices","result":{"pageContext":{"content":"<p>Machine Learning on Coursera week 1 exercise includes linear regression to predict housing prices. The data contains the size and number of bedrooms in a house, and the price for that house. Our goal is to use this data so that given the size and number of bedrooms, we are able to predict the price of the house.</p>\n<h2>Part 1: Getting the Data</h2>\n<pre><code class=\"language-python\">import matplotlib.pyplot as plt\nimport numpy as np\n\npath = '/Users/archit-p/sources/coursera/machine-learning/machine-learning-ex1/ex1/ex1data2.txt'\n</code></pre>\n<pre><code class=\"language-python\">f = open(path, 'r')\n\ndataset = []\n\nfor line in f:\n    fields = line.strip().split(',')\n    fields[0] = float(fields[0])\n    fields[1] = float(fields[1])\n    fields[2] = float(fields[2])\n    dataset.append(fields)\n</code></pre>\n<pre><code class=\"language-python\">dataset[0:5]\n</code></pre>\n<pre><code>[[2104.0, 3.0, 399900.0],\n [1600.0, 3.0, 329900.0],\n [2400.0, 3.0, 369000.0],\n [1416.0, 2.0, 232000.0],\n [3000.0, 4.0, 539900.0]]\n</code></pre>\n<p>In order to make gradient descent converge faster, we normalize the data. To normalize the data, we subtract the mean and divide by the standard deviation. This gives values roughly in the range of <code>[-1, +1]</code>.</p>\n<pre><code class=\"language-python\"># number of features\nn = 2\n\n# number of datapoints\nm = len(dataset)\n\nx = np.ones(shape=(m, n + 1))\ny = np.zeros(m)\n\nfor i, d in enumerate(dataset):\n    for j in range(n):\n        x[i][1 + j] = dataset[i][j]\n        y[i] = dataset[i][-1]\n\n# normalization        \nfor j in range(n):\n    x[:, 1 + j] = (x[:, 1 + j] - np.ones(m) * x[:, 1 + j].mean()) / x[:, 1 + j].std()\n</code></pre>\n<pre><code class=\"language-python\">x[0:5]\n</code></pre>\n<pre><code>array([[ 1.        ,  0.13141542, -0.22609337],\n       [ 1.        , -0.5096407 , -0.22609337],\n       [ 1.        ,  0.5079087 , -0.22609337],\n       [ 1.        , -0.74367706, -1.5543919 ],\n       [ 1.        ,  1.27107075,  1.10220517]])\n</code></pre>\n<pre><code class=\"language-python\">y[0:5]\n</code></pre>\n<pre><code>array([399900., 329900., 369000., 232000., 539900.])\n</code></pre>\n<p>To visualize the data, we plot a 3-dimensional scatter plot.</p>\n<pre><code class=\"language-python\">fig = plt.figure(figsize=(6, 6))\nax = plt.axes(projection='3d')\n\nax.scatter(x[:, 1], x[:, 2], y)\nplt.show()\n</code></pre>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 363px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c7eb86f631ccd8eac9dec681650630d4/4e786/output_10_0.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 93.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC4ElEQVQ4y12UyW7UQBCGRyC2F+MNeAheIuQGinIDgQSIIyhHuHBC0RyiRIEDCYdkEk2WIfHWttvOZJlJ97j4/4rbmWCpXN1291d/VS89EXkB+9U0zQ/YKtqr9KEdLHybt/Y75/2EPUe/18PrKzpC4xM8H++9OOfufAtPmDM397UC0fnSfpjAe3JoV1dXvqoqXxSFt9b68Xis3xDAz49r58lsNlsKClfaKI5U/JDz83MBTM7OzqQsSwFU8jxXY5v/EEAQoJl571rRS4PBQBWutB/cdDpVCCfUda0WgDTbtqFY4WVRNFGau5PTSLa3t971er0HncLLy0sXVBFUVbW2AywYQQSyjWBNagp/dHQsGxsb7wF8okDACHEEMRUF1wTeKqS64UkqSWbw31KdAq0t/eHhoaytrX0E8LECUYsOGBRmBSYBnOc6UQajRP4mRoFUeJJkAnUdcHNz8wOAjxQ4mUyoygWY1g6TavSPokz+HERimCrAoziTvCjFGCwQFCKY39/fl36//wnAh7dAawEca6rWVpLkpZwaK1vDSLYOYqjJFWRvUg21VOBwOJT19XWmfE+BXF38dKyfRZqjpJDvv49lF2lGWYH0cgbsthEzaFdbgXt7e1RI4P07QE7Iy0piU8rucax1HNeVmKLstlCtGdiwTxuA3Wg0kp2dnbfdxr6+vr5ZFKRsAMkZnXsRZlXZXZht1SLtJo5jx+PJjZ2m6Q1wihpGqXGJKbSGmrpOrDplBMzDmHKSJA12iOO2w7lf7hTyuHGVEU0yYyTLMlUYIMGHjd/CeBgUyPl4lsPloEePJwVRmouLi4bFNsY0URSxRix8A6UNzrj2qYwwjvXtWQZnGabAz+1VNeF5xiAOcNhKPNs8jg61cQjgAFLjGFoLm7S3VXfbfPv/vmOfC8WHp4h9ADRlelr4P3ePvgkpP0PjFfwC/CL8In1oQ4W2UTv1CLCI1LWN2tEvYNxL+KdM+R/SDXISHWxP/QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/c7eb86f631ccd8eac9dec681650630d4/4e786/output_10_0.png\"\n        srcset=\"/static/c7eb86f631ccd8eac9dec681650630d4/772e8/output_10_0.png 200w,\n/static/c7eb86f631ccd8eac9dec681650630d4/4e786/output_10_0.png 363w\"\n        sizes=\"(max-width: 363px) 100vw, 363px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2>Part 2: Regression and Gradient Descent</h2>\n<p>In Part 2, we define the hypothesis, error and gradient functions and perform the minimization.</p>\n<pre><code class=\"language-python\">def hypothesis(theta, xi):\n    return np.matmul(np.transpose(theta), xi)\n</code></pre>\n<pre><code class=\"language-python\">def mse(theta):\n    sum = 0\n    predictions = np.zeros(m)\n    for i, xi in enumerate(x):\n        predictions[i] = hypothesis(theta, xi)\n    return ((predictions - y) ** 2).mean() / 2;\n</code></pre>\n<pre><code class=\"language-python\">mse(np.zeros(n + 1))\n</code></pre>\n<pre><code>65591548106.45744\n</code></pre>\n<pre><code class=\"language-python\">def derivative(theta, j):\n    sum = 0\n    for xi, yi in zip(x, y):\n        sum += (hypothesis(theta, xi) - yi) * xi[j]\n    return sum / m\n</code></pre>\n<pre><code class=\"language-python\">derivative(np.zeros(n + 1), 0)\n</code></pre>\n<pre><code>-340412.6595744681\n</code></pre>\n<p>To minimize the error, we use gradient descent with a learning rate of 0.02.</p>\n<pre><code class=\"language-python\">%%time\ntheta = np.zeros(n + 1)\niterations = 2000\nalpha = 0.02\n\nh = np.zeros(iterations)\n\nfor i in range(iterations):\n    newtheta = np.zeros(n + 1)\n    for j in range(n + 1):\n        newtheta[j] = theta[j] - alpha * derivative(theta, j)\n    theta = newtheta\n    h[i] = mse(theta)\n\nprint(theta)\n</code></pre>\n<pre><code>[340412.65957447 109447.79525195  -6578.35363647]\nCPU times: user 1.12 s, sys: 7.42 ms, total: 1.12 s\nWall time: 1.13 s\n</code></pre>\n<p>In order to check whether gradient descent is converging and we have made a correct choice of alpha. We plot the error values vs the number of iterations. In an ideal scenario this value should decrease and converge to zero with the number of iterations.</p>\n<pre><code class=\"language-python\">plt.plot(h)\nplt.show()\n</code></pre>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 362px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/279c0200858e850a9eeff15cacd5f5be/10600/output_20_0.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.50000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAABVElEQVQ4y62TzUrDQBDH1yp+IHgRD4JQ38EnEBQvvoAi+D727FH6Au0LiCBWmxYRQUN7LCahNdkmuw2V2uZrnVk2hHqRxA78+Gd3dv4ZkllyeXFOKpWr03q9dtbr9Q51XT/udDp5Oep2uyeaph0QiBVz4LzOokQIkYgkKQaG53lNNCyBYWM8jXAvgCQ+5AJqAixmjN3KDvu20/S/Q6GSRSJSHd4RUEJd1uCTIDUUeWPOEGJ1YDvaKOvw34bEGbqPo8niDJdtOnxapOHS7w7zmv7ZYQHmDEs2dR/UX57B+3LPIdQEylDOIfH90YsPgz2ehiKME4G3JogylaT7cbbO9mMRgyHnXN6UNdMwbvzx15tFWcuwvWeTsjaqRblU02Htj09XrlHTNSrmDZu1+kP+blnWNbmvVeUs4o0BNoEdYAPYxc+hdB3YV1pW+bI6j/ktYA/Y/gE45LB7MVE0SgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"png\"\n        title=\"png\"\n        src=\"/static/279c0200858e850a9eeff15cacd5f5be/10600/output_20_0.png\"\n        srcset=\"/static/279c0200858e850a9eeff15cacd5f5be/772e8/output_20_0.png 200w,\n/static/279c0200858e850a9eeff15cacd5f5be/10600/output_20_0.png 362w\"\n        sizes=\"(max-width: 362px) 100vw, 362px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>","frontmatter":{"slug":"/blog/linear-regression-housing-prices","title":"Linear Regression to Predict Housing Prices","date":"February 19th 2021"}}},"staticQueryHashes":[]}